{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Linear Regression in the PyTorch way.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfpeIrGxcmlVfpnrOZ035y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeopjjang/ML-Study/blob/main/4_Linear_Regression_in_the_PyTorch_way.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBpovhZRXLvm"
      },
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "from torch import tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvYrLBxzYDgh",
        "outputId": "d3ab6f92-0d5c-4ada-c208-dab330483878"
      },
      "source": [
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QojX5A3bYakw",
        "outputId": "384eff7f-f0d6-4d48-f7b2-707f76e99800"
      },
      "source": [
        "## Model class in PyTorch way\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear module\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1) # One in and one out\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must return\n",
        "    a Variable of output data. We can Modules defined in the constructor as\n",
        "    well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJlXj84KZ_7M"
      },
      "source": [
        "## Construct loss and optimizer\n",
        "\"\"\"\n",
        "Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "in the SGD constructor will contain the learnable parameters of the two\n",
        "nn.Linear modules which are members of the model.\n",
        "\"\"\"\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHATVnJTaquy",
        "outputId": "e77ea86b-517c-4858-f530-d330e0f65b74"
      },
      "source": [
        "## Training: forward, loss backward, step\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "  # Forward pass: Compute predicted y by passing x to the model\n",
        "  y_pred = model(x_data)\n",
        "\n",
        "  # Compute and print loss\n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "  # Zero gradients, perform a backward pass, and update the weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 0.00028627095161937177\n",
            "Epoch: 1 | Loss: 0.0002821616653818637\n",
            "Epoch: 2 | Loss: 0.00027810438768938184\n",
            "Epoch: 3 | Loss: 0.0002741178614087403\n",
            "Epoch: 4 | Loss: 0.0002701773482840508\n",
            "Epoch: 5 | Loss: 0.00026628340128809214\n",
            "Epoch: 6 | Loss: 0.00026246198103763163\n",
            "Epoch: 7 | Loss: 0.000258688727626577\n",
            "Epoch: 8 | Loss: 0.0002549762139096856\n",
            "Epoch: 9 | Loss: 0.00025130624999292195\n",
            "Epoch: 10 | Loss: 0.0002476958034094423\n",
            "Epoch: 11 | Loss: 0.00024414055224042386\n",
            "Epoch: 12 | Loss: 0.00024062364536803216\n",
            "Epoch: 13 | Loss: 0.00023717162548564374\n",
            "Epoch: 14 | Loss: 0.0002337641199119389\n",
            "Epoch: 15 | Loss: 0.0002303998189745471\n",
            "Epoch: 16 | Loss: 0.0002270835539093241\n",
            "Epoch: 17 | Loss: 0.00022382783936336637\n",
            "Epoch: 18 | Loss: 0.00022060773335397243\n",
            "Epoch: 19 | Loss: 0.00021743755496572703\n",
            "Epoch: 20 | Loss: 0.0002143090241588652\n",
            "Epoch: 21 | Loss: 0.0002112369693350047\n",
            "Epoch: 22 | Loss: 0.00020819803467020392\n",
            "Epoch: 23 | Loss: 0.00020521023543551564\n",
            "Epoch: 24 | Loss: 0.0002022597473114729\n",
            "Epoch: 25 | Loss: 0.00019935195450671017\n",
            "Epoch: 26 | Loss: 0.000196479115402326\n",
            "Epoch: 27 | Loss: 0.00019365701882634312\n",
            "Epoch: 28 | Loss: 0.00019087301916442811\n",
            "Epoch: 29 | Loss: 0.00018814030045177788\n",
            "Epoch: 30 | Loss: 0.00018543440091889352\n",
            "Epoch: 31 | Loss: 0.00018276931950822473\n",
            "Epoch: 32 | Loss: 0.000180140690645203\n",
            "Epoch: 33 | Loss: 0.00017754893633536994\n",
            "Epoch: 34 | Loss: 0.00017499903333373368\n",
            "Epoch: 35 | Loss: 0.0001724782632663846\n",
            "Epoch: 36 | Loss: 0.00017000004299916327\n",
            "Epoch: 37 | Loss: 0.00016756232071202248\n",
            "Epoch: 38 | Loss: 0.0001651519414735958\n",
            "Epoch: 39 | Loss: 0.00016277533723041415\n",
            "Epoch: 40 | Loss: 0.00016043952200561762\n",
            "Epoch: 41 | Loss: 0.00015813081699889153\n",
            "Epoch: 42 | Loss: 0.0001558606163598597\n",
            "Epoch: 43 | Loss: 0.00015361692931037396\n",
            "Epoch: 44 | Loss: 0.00015141240146476775\n",
            "Epoch: 45 | Loss: 0.00014923943672329187\n",
            "Epoch: 46 | Loss: 0.00014709134120494127\n",
            "Epoch: 47 | Loss: 0.00014498192467726767\n",
            "Epoch: 48 | Loss: 0.00014289538376033306\n",
            "Epoch: 49 | Loss: 0.00014084122085478157\n",
            "Epoch: 50 | Loss: 0.00013881700579077005\n",
            "Epoch: 51 | Loss: 0.00013682171993423253\n",
            "Epoch: 52 | Loss: 0.00013485774979926646\n",
            "Epoch: 53 | Loss: 0.00013291736831888556\n",
            "Epoch: 54 | Loss: 0.00013100504293106496\n",
            "Epoch: 55 | Loss: 0.00012912174861412495\n",
            "Epoch: 56 | Loss: 0.00012727308785542846\n",
            "Epoch: 57 | Loss: 0.00012543710181489587\n",
            "Epoch: 58 | Loss: 0.0001236415992025286\n",
            "Epoch: 59 | Loss: 0.00012186161620775238\n",
            "Epoch: 60 | Loss: 0.00012010665523121133\n",
            "Epoch: 61 | Loss: 0.00011837895726785064\n",
            "Epoch: 62 | Loss: 0.00011668514343909919\n",
            "Epoch: 63 | Loss: 0.0001150060270447284\n",
            "Epoch: 64 | Loss: 0.00011335269664414227\n",
            "Epoch: 65 | Loss: 0.00011172484664712101\n",
            "Epoch: 66 | Loss: 0.00011011428432539105\n",
            "Epoch: 67 | Loss: 0.00010853356798179448\n",
            "Epoch: 68 | Loss: 0.00010697451943997294\n",
            "Epoch: 69 | Loss: 0.00010543807729845867\n",
            "Epoch: 70 | Loss: 0.00010392163676442578\n",
            "Epoch: 71 | Loss: 0.00010243032011203468\n",
            "Epoch: 72 | Loss: 0.00010095328616444021\n",
            "Epoch: 73 | Loss: 9.950494859367609e-05\n",
            "Epoch: 74 | Loss: 9.807915921555832e-05\n",
            "Epoch: 75 | Loss: 9.666482219472528e-05\n",
            "Epoch: 76 | Loss: 9.527778456686065e-05\n",
            "Epoch: 77 | Loss: 9.390864579472691e-05\n",
            "Epoch: 78 | Loss: 9.255725308321416e-05\n",
            "Epoch: 79 | Loss: 9.122398478211835e-05\n",
            "Epoch: 80 | Loss: 8.991251525003463e-05\n",
            "Epoch: 81 | Loss: 8.862805407261476e-05\n",
            "Epoch: 82 | Loss: 8.735230221645907e-05\n",
            "Epoch: 83 | Loss: 8.609388169134036e-05\n",
            "Epoch: 84 | Loss: 8.485637954436243e-05\n",
            "Epoch: 85 | Loss: 8.363634697161615e-05\n",
            "Epoch: 86 | Loss: 8.243358752224594e-05\n",
            "Epoch: 87 | Loss: 8.125316526275128e-05\n",
            "Epoch: 88 | Loss: 8.00838679424487e-05\n",
            "Epoch: 89 | Loss: 7.893284782767296e-05\n",
            "Epoch: 90 | Loss: 7.779785664752126e-05\n",
            "Epoch: 91 | Loss: 7.668331090826541e-05\n",
            "Epoch: 92 | Loss: 7.557528442703187e-05\n",
            "Epoch: 93 | Loss: 7.449137774528936e-05\n",
            "Epoch: 94 | Loss: 7.34187924535945e-05\n",
            "Epoch: 95 | Loss: 7.236635428853333e-05\n",
            "Epoch: 96 | Loss: 7.132739847293124e-05\n",
            "Epoch: 97 | Loss: 7.030034612398595e-05\n",
            "Epoch: 98 | Loss: 6.929139635758474e-05\n",
            "Epoch: 99 | Loss: 6.829212361481041e-05\n",
            "Epoch: 100 | Loss: 6.731157191097736e-05\n",
            "Epoch: 101 | Loss: 6.634663441218436e-05\n",
            "Epoch: 102 | Loss: 6.539383321069181e-05\n",
            "Epoch: 103 | Loss: 6.445492181228474e-05\n",
            "Epoch: 104 | Loss: 6.353067874442786e-05\n",
            "Epoch: 105 | Loss: 6.261036469368264e-05\n",
            "Epoch: 106 | Loss: 6.171638960950077e-05\n",
            "Epoch: 107 | Loss: 6.0824771935585886e-05\n",
            "Epoch: 108 | Loss: 5.9953585150651634e-05\n",
            "Epoch: 109 | Loss: 5.908913954044692e-05\n",
            "Epoch: 110 | Loss: 5.824294566991739e-05\n",
            "Epoch: 111 | Loss: 5.740506458096206e-05\n",
            "Epoch: 112 | Loss: 5.657718429574743e-05\n",
            "Epoch: 113 | Loss: 5.576617695624009e-05\n",
            "Epoch: 114 | Loss: 5.4965330491540954e-05\n",
            "Epoch: 115 | Loss: 5.4178410209715366e-05\n",
            "Epoch: 116 | Loss: 5.339461495168507e-05\n",
            "Epoch: 117 | Loss: 5.262960257823579e-05\n",
            "Epoch: 118 | Loss: 5.187178612686694e-05\n",
            "Epoch: 119 | Loss: 5.112653525429778e-05\n",
            "Epoch: 120 | Loss: 5.0393697165418416e-05\n",
            "Epoch: 121 | Loss: 4.9668189603835344e-05\n",
            "Epoch: 122 | Loss: 4.895364327239804e-05\n",
            "Epoch: 123 | Loss: 4.825114592676982e-05\n",
            "Epoch: 124 | Loss: 4.755692134494893e-05\n",
            "Epoch: 125 | Loss: 4.687330510932952e-05\n",
            "Epoch: 126 | Loss: 4.6200177166610956e-05\n",
            "Epoch: 127 | Loss: 4.5535049139289185e-05\n",
            "Epoch: 128 | Loss: 4.488331251195632e-05\n",
            "Epoch: 129 | Loss: 4.423705831868574e-05\n",
            "Epoch: 130 | Loss: 4.360008460935205e-05\n",
            "Epoch: 131 | Loss: 4.297459599911235e-05\n",
            "Epoch: 132 | Loss: 4.23559031332843e-05\n",
            "Epoch: 133 | Loss: 4.1747320210561156e-05\n",
            "Epoch: 134 | Loss: 4.1148741729557514e-05\n",
            "Epoch: 135 | Loss: 4.0554114093538374e-05\n",
            "Epoch: 136 | Loss: 3.997483872808516e-05\n",
            "Epoch: 137 | Loss: 3.939754969906062e-05\n",
            "Epoch: 138 | Loss: 3.8832058635307476e-05\n",
            "Epoch: 139 | Loss: 3.827389809885062e-05\n",
            "Epoch: 140 | Loss: 3.772477793972939e-05\n",
            "Epoch: 141 | Loss: 3.7182460800977424e-05\n",
            "Epoch: 142 | Loss: 3.66493477486074e-05\n",
            "Epoch: 143 | Loss: 3.6119392461841926e-05\n",
            "Epoch: 144 | Loss: 3.560231198207475e-05\n",
            "Epoch: 145 | Loss: 3.509033558657393e-05\n",
            "Epoch: 146 | Loss: 3.458822175161913e-05\n",
            "Epoch: 147 | Loss: 3.4089393011527136e-05\n",
            "Epoch: 148 | Loss: 3.359822585480288e-05\n",
            "Epoch: 149 | Loss: 3.311899126856588e-05\n",
            "Epoch: 150 | Loss: 3.263888356741518e-05\n",
            "Epoch: 151 | Loss: 3.217051562387496e-05\n",
            "Epoch: 152 | Loss: 3.170947093167342e-05\n",
            "Epoch: 153 | Loss: 3.125337389064953e-05\n",
            "Epoch: 154 | Loss: 3.080509850406088e-05\n",
            "Epoch: 155 | Loss: 3.0360706659848802e-05\n",
            "Epoch: 156 | Loss: 2.9926217393949628e-05\n",
            "Epoch: 157 | Loss: 2.949517875094898e-05\n",
            "Epoch: 158 | Loss: 2.907291127485223e-05\n",
            "Epoch: 159 | Loss: 2.8652444598264992e-05\n",
            "Epoch: 160 | Loss: 2.824245166266337e-05\n",
            "Epoch: 161 | Loss: 2.7834801585413516e-05\n",
            "Epoch: 162 | Loss: 2.7435598894953728e-05\n",
            "Epoch: 163 | Loss: 2.704048529267311e-05\n",
            "Epoch: 164 | Loss: 2.6651543521438725e-05\n",
            "Epoch: 165 | Loss: 2.626869172672741e-05\n",
            "Epoch: 166 | Loss: 2.5891573386616074e-05\n",
            "Epoch: 167 | Loss: 2.5521876523271203e-05\n",
            "Epoch: 168 | Loss: 2.5152217858703807e-05\n",
            "Epoch: 169 | Loss: 2.479162321833428e-05\n",
            "Epoch: 170 | Loss: 2.4435645173070952e-05\n",
            "Epoch: 171 | Loss: 2.4085380573524162e-05\n",
            "Epoch: 172 | Loss: 2.3738493837299757e-05\n",
            "Epoch: 173 | Loss: 2.3397216864395887e-05\n",
            "Epoch: 174 | Loss: 2.306175883859396e-05\n",
            "Epoch: 175 | Loss: 2.2729282136424445e-05\n",
            "Epoch: 176 | Loss: 2.2401418391382322e-05\n",
            "Epoch: 177 | Loss: 2.20797628571745e-05\n",
            "Epoch: 178 | Loss: 2.1765852579846978e-05\n",
            "Epoch: 179 | Loss: 2.1449352061608806e-05\n",
            "Epoch: 180 | Loss: 2.114238122885581e-05\n",
            "Epoch: 181 | Loss: 2.0837625925196335e-05\n",
            "Epoch: 182 | Loss: 2.0539031538646668e-05\n",
            "Epoch: 183 | Loss: 2.0245210180291906e-05\n",
            "Epoch: 184 | Loss: 1.995246930164285e-05\n",
            "Epoch: 185 | Loss: 1.9666498701553792e-05\n",
            "Epoch: 186 | Loss: 1.938464083650615e-05\n",
            "Epoch: 187 | Loss: 1.9106344552710652e-05\n",
            "Epoch: 188 | Loss: 1.8829552573151886e-05\n",
            "Epoch: 189 | Loss: 1.855903974501416e-05\n",
            "Epoch: 190 | Loss: 1.82929725269787e-05\n",
            "Epoch: 191 | Loss: 1.8031785657512955e-05\n",
            "Epoch: 192 | Loss: 1.7772477804101072e-05\n",
            "Epoch: 193 | Loss: 1.751480704115238e-05\n",
            "Epoch: 194 | Loss: 1.726457048789598e-05\n",
            "Epoch: 195 | Loss: 1.7014699551509693e-05\n",
            "Epoch: 196 | Loss: 1.677045838732738e-05\n",
            "Epoch: 197 | Loss: 1.652892751735635e-05\n",
            "Epoch: 198 | Loss: 1.629337202757597e-05\n",
            "Epoch: 199 | Loss: 1.6059275367297232e-05\n",
            "Epoch: 200 | Loss: 1.5828492905711755e-05\n",
            "Epoch: 201 | Loss: 1.5601217455696315e-05\n",
            "Epoch: 202 | Loss: 1.5377179806819186e-05\n",
            "Epoch: 203 | Loss: 1.5156349036260508e-05\n",
            "Epoch: 204 | Loss: 1.4937114428903442e-05\n",
            "Epoch: 205 | Loss: 1.4721039406140335e-05\n",
            "Epoch: 206 | Loss: 1.4510525033983868e-05\n",
            "Epoch: 207 | Loss: 1.4302405361377168e-05\n",
            "Epoch: 208 | Loss: 1.4097535313339904e-05\n",
            "Epoch: 209 | Loss: 1.389284534525359e-05\n",
            "Epoch: 210 | Loss: 1.3694599147129338e-05\n",
            "Epoch: 211 | Loss: 1.3497777217708062e-05\n",
            "Epoch: 212 | Loss: 1.3303228115546517e-05\n",
            "Epoch: 213 | Loss: 1.311219693889143e-05\n",
            "Epoch: 214 | Loss: 1.292401157115819e-05\n",
            "Epoch: 215 | Loss: 1.2737810720864218e-05\n",
            "Epoch: 216 | Loss: 1.2555429748317692e-05\n",
            "Epoch: 217 | Loss: 1.2374161997286137e-05\n",
            "Epoch: 218 | Loss: 1.219765908899717e-05\n",
            "Epoch: 219 | Loss: 1.2020413123536855e-05\n",
            "Epoch: 220 | Loss: 1.1849666407215409e-05\n",
            "Epoch: 221 | Loss: 1.1676965186779853e-05\n",
            "Epoch: 222 | Loss: 1.1509668183862232e-05\n",
            "Epoch: 223 | Loss: 1.1344949598424137e-05\n",
            "Epoch: 224 | Loss: 1.1183170499862172e-05\n",
            "Epoch: 225 | Loss: 1.1022553735529073e-05\n",
            "Epoch: 226 | Loss: 1.086309930542484e-05\n",
            "Epoch: 227 | Loss: 1.0705754903028719e-05\n",
            "Epoch: 228 | Loss: 1.0552770618232898e-05\n",
            "Epoch: 229 | Loss: 1.0400887731520925e-05\n",
            "Epoch: 230 | Loss: 1.0251967978547327e-05\n",
            "Epoch: 231 | Loss: 1.0103567547048442e-05\n",
            "Epoch: 232 | Loss: 9.958999726222828e-06\n",
            "Epoch: 233 | Loss: 9.815293196879793e-06\n",
            "Epoch: 234 | Loss: 9.67552205111133e-06\n",
            "Epoch: 235 | Loss: 9.535857316222973e-06\n",
            "Epoch: 236 | Loss: 9.399345799465664e-06\n",
            "Epoch: 237 | Loss: 9.263999345421325e-06\n",
            "Epoch: 238 | Loss: 9.130684702540748e-06\n",
            "Epoch: 239 | Loss: 9.000081263366155e-06\n",
            "Epoch: 240 | Loss: 8.870247256709263e-06\n",
            "Epoch: 241 | Loss: 8.742385034565814e-06\n",
            "Epoch: 242 | Loss: 8.616989362053573e-06\n",
            "Epoch: 243 | Loss: 8.493853783875238e-06\n",
            "Epoch: 244 | Loss: 8.371437616006006e-06\n",
            "Epoch: 245 | Loss: 8.250743121607229e-06\n",
            "Epoch: 246 | Loss: 8.133082701533567e-06\n",
            "Epoch: 247 | Loss: 8.015772436920088e-06\n",
            "Epoch: 248 | Loss: 7.900458513177e-06\n",
            "Epoch: 249 | Loss: 7.787116373947356e-06\n",
            "Epoch: 250 | Loss: 7.67507663113065e-06\n",
            "Epoch: 251 | Loss: 7.5646466939360835e-06\n",
            "Epoch: 252 | Loss: 7.45485976949567e-06\n",
            "Epoch: 253 | Loss: 7.34760897103115e-06\n",
            "Epoch: 254 | Loss: 7.243011168611702e-06\n",
            "Epoch: 255 | Loss: 7.139475201256573e-06\n",
            "Epoch: 256 | Loss: 7.036222086753696e-06\n",
            "Epoch: 257 | Loss: 6.935404599062167e-06\n",
            "Epoch: 258 | Loss: 6.836228749307338e-06\n",
            "Epoch: 259 | Loss: 6.737462172168307e-06\n",
            "Epoch: 260 | Loss: 6.639716957579367e-06\n",
            "Epoch: 261 | Loss: 6.545063115481753e-06\n",
            "Epoch: 262 | Loss: 6.45094314677408e-06\n",
            "Epoch: 263 | Loss: 6.357504389598034e-06\n",
            "Epoch: 264 | Loss: 6.266929631237872e-06\n",
            "Epoch: 265 | Loss: 6.176861461426597e-06\n",
            "Epoch: 266 | Loss: 6.087445399316493e-06\n",
            "Epoch: 267 | Loss: 5.999819222779479e-06\n",
            "Epoch: 268 | Loss: 5.913959284953307e-06\n",
            "Epoch: 269 | Loss: 5.829982910654508e-06\n",
            "Epoch: 270 | Loss: 5.74521254748106e-06\n",
            "Epoch: 271 | Loss: 5.6631379266036674e-06\n",
            "Epoch: 272 | Loss: 5.5819300541770644e-06\n",
            "Epoch: 273 | Loss: 5.500898623722605e-06\n",
            "Epoch: 274 | Loss: 5.422897629614454e-06\n",
            "Epoch: 275 | Loss: 5.344107194105163e-06\n",
            "Epoch: 276 | Loss: 5.267097549221944e-06\n",
            "Epoch: 277 | Loss: 5.191437594476156e-06\n",
            "Epoch: 278 | Loss: 5.117512046126649e-06\n",
            "Epoch: 279 | Loss: 5.0441158236935735e-06\n",
            "Epoch: 280 | Loss: 4.971249836671632e-06\n",
            "Epoch: 281 | Loss: 4.899942723568529e-06\n",
            "Epoch: 282 | Loss: 4.830045782000525e-06\n",
            "Epoch: 283 | Loss: 4.760269348480506e-06\n",
            "Epoch: 284 | Loss: 4.691632966569159e-06\n",
            "Epoch: 285 | Loss: 4.624118446372449e-06\n",
            "Epoch: 286 | Loss: 4.55771214546985e-06\n",
            "Epoch: 287 | Loss: 4.492404968914343e-06\n",
            "Epoch: 288 | Loss: 4.427201020007487e-06\n",
            "Epoch: 289 | Loss: 4.363324478617869e-06\n",
            "Epoch: 290 | Loss: 4.300995442463318e-06\n",
            "Epoch: 291 | Loss: 4.238877409079578e-06\n",
            "Epoch: 292 | Loss: 4.178278686595149e-06\n",
            "Epoch: 293 | Loss: 4.118825927434955e-06\n",
            "Epoch: 294 | Loss: 4.05921400670195e-06\n",
            "Epoch: 295 | Loss: 4.000967237516306e-06\n",
            "Epoch: 296 | Loss: 3.943716365029104e-06\n",
            "Epoch: 297 | Loss: 3.887109414790757e-06\n",
            "Epoch: 298 | Loss: 3.830569767160341e-06\n",
            "Epoch: 299 | Loss: 3.7756867641292047e-06\n",
            "Epoch: 300 | Loss: 3.7207514651527163e-06\n",
            "Epoch: 301 | Loss: 3.66833342013706e-06\n",
            "Epoch: 302 | Loss: 3.6152937354927417e-06\n",
            "Epoch: 303 | Loss: 3.5640653095470043e-06\n",
            "Epoch: 304 | Loss: 3.5117875540890964e-06\n",
            "Epoch: 305 | Loss: 3.4615181903063785e-06\n",
            "Epoch: 306 | Loss: 3.4108593354176264e-06\n",
            "Epoch: 307 | Loss: 3.362918732818798e-06\n",
            "Epoch: 308 | Loss: 3.3142582651635166e-06\n",
            "Epoch: 309 | Loss: 3.2661639579600887e-06\n",
            "Epoch: 310 | Loss: 3.2192549497267464e-06\n",
            "Epoch: 311 | Loss: 3.1734105050418293e-06\n",
            "Epoch: 312 | Loss: 3.127894842691603e-06\n",
            "Epoch: 313 | Loss: 3.0837295525998343e-06\n",
            "Epoch: 314 | Loss: 3.038053819182096e-06\n",
            "Epoch: 315 | Loss: 2.9950315365567803e-06\n",
            "Epoch: 316 | Loss: 2.9522170734708197e-06\n",
            "Epoch: 317 | Loss: 2.9090165298839565e-06\n",
            "Epoch: 318 | Loss: 2.8671195195784094e-06\n",
            "Epoch: 319 | Loss: 2.826795252985903e-06\n",
            "Epoch: 320 | Loss: 2.785884362310753e-06\n",
            "Epoch: 321 | Loss: 2.7468126972962637e-06\n",
            "Epoch: 322 | Loss: 2.7060093543695984e-06\n",
            "Epoch: 323 | Loss: 2.6676946163206594e-06\n",
            "Epoch: 324 | Loss: 2.6293696464563254e-06\n",
            "Epoch: 325 | Loss: 2.5916033337125555e-06\n",
            "Epoch: 326 | Loss: 2.5538308818795485e-06\n",
            "Epoch: 327 | Loss: 2.5179963358823443e-06\n",
            "Epoch: 328 | Loss: 2.4814073640300194e-06\n",
            "Epoch: 329 | Loss: 2.4460864551656414e-06\n",
            "Epoch: 330 | Loss: 2.410929027973907e-06\n",
            "Epoch: 331 | Loss: 2.375756821493269e-06\n",
            "Epoch: 332 | Loss: 2.341821527807042e-06\n",
            "Epoch: 333 | Loss: 2.308130433448241e-06\n",
            "Epoch: 334 | Loss: 2.2745964542991715e-06\n",
            "Epoch: 335 | Loss: 2.2417420950660016e-06\n",
            "Epoch: 336 | Loss: 2.2106826236267807e-06\n",
            "Epoch: 337 | Loss: 2.1778666905447608e-06\n",
            "Epoch: 338 | Loss: 2.1472544631251367e-06\n",
            "Epoch: 339 | Loss: 2.1160990399948787e-06\n",
            "Epoch: 340 | Loss: 2.0852544366789516e-06\n",
            "Epoch: 341 | Loss: 2.0559687072818633e-06\n",
            "Epoch: 342 | Loss: 2.026146376010729e-06\n",
            "Epoch: 343 | Loss: 1.997117351493216e-06\n",
            "Epoch: 344 | Loss: 1.9683786831592442e-06\n",
            "Epoch: 345 | Loss: 1.939768026204547e-06\n",
            "Epoch: 346 | Loss: 1.912089146571816e-06\n",
            "Epoch: 347 | Loss: 1.8844507394533139e-06\n",
            "Epoch: 348 | Loss: 1.8570922293292824e-06\n",
            "Epoch: 349 | Loss: 1.83072074833035e-06\n",
            "Epoch: 350 | Loss: 1.8049279333354207e-06\n",
            "Epoch: 351 | Loss: 1.7786207990866387e-06\n",
            "Epoch: 352 | Loss: 1.7528145690448582e-06\n",
            "Epoch: 353 | Loss: 1.7281133750657318e-06\n",
            "Epoch: 354 | Loss: 1.7029808532242896e-06\n",
            "Epoch: 355 | Loss: 1.6787862477940507e-06\n",
            "Epoch: 356 | Loss: 1.6546141523576807e-06\n",
            "Epoch: 357 | Loss: 1.6311373656208161e-06\n",
            "Epoch: 358 | Loss: 1.607460262675886e-06\n",
            "Epoch: 359 | Loss: 1.5839564184716437e-06\n",
            "Epoch: 360 | Loss: 1.5614966741850367e-06\n",
            "Epoch: 361 | Loss: 1.53941414282599e-06\n",
            "Epoch: 362 | Loss: 1.516772272225353e-06\n",
            "Epoch: 363 | Loss: 1.494938942414592e-06\n",
            "Epoch: 364 | Loss: 1.4736858702235622e-06\n",
            "Epoch: 365 | Loss: 1.4522354376822477e-06\n",
            "Epoch: 366 | Loss: 1.431220425729407e-06\n",
            "Epoch: 367 | Loss: 1.4116017155174632e-06\n",
            "Epoch: 368 | Loss: 1.3910896541347029e-06\n",
            "Epoch: 369 | Loss: 1.3712044619751396e-06\n",
            "Epoch: 370 | Loss: 1.3514625152311055e-06\n",
            "Epoch: 371 | Loss: 1.3318638139026007e-06\n",
            "Epoch: 372 | Loss: 1.3127403235557722e-06\n",
            "Epoch: 373 | Loss: 1.2934253845742205e-06\n",
            "Epoch: 374 | Loss: 1.2747134405799443e-06\n",
            "Epoch: 375 | Loss: 1.2566583791340236e-06\n",
            "Epoch: 376 | Loss: 1.2387322385620791e-06\n",
            "Epoch: 377 | Loss: 1.2209347914904356e-06\n",
            "Epoch: 378 | Loss: 1.2032662652927684e-06\n",
            "Epoch: 379 | Loss: 1.1859794994961703e-06\n",
            "Epoch: 380 | Loss: 1.1690692645061063e-06\n",
            "Epoch: 381 | Loss: 1.1521563010319369e-06\n",
            "Epoch: 382 | Loss: 1.1356144113960909e-06\n",
            "Epoch: 383 | Loss: 1.1194379112566821e-06\n",
            "Epoch: 384 | Loss: 1.1029499091819162e-06\n",
            "Epoch: 385 | Loss: 1.087796135834651e-06\n",
            "Epoch: 386 | Loss: 1.0716644283093046e-06\n",
            "Epoch: 387 | Loss: 1.0563701380306156e-06\n",
            "Epoch: 388 | Loss: 1.0407704849058064e-06\n",
            "Epoch: 389 | Loss: 1.0266405752190622e-06\n",
            "Epoch: 390 | Loss: 1.01126261142781e-06\n",
            "Epoch: 391 | Loss: 9.96988092083484e-07\n",
            "Epoch: 392 | Loss: 9.822952051763423e-07\n",
            "Epoch: 393 | Loss: 9.685127224656753e-07\n",
            "Epoch: 394 | Loss: 9.544298791297479e-07\n",
            "Epoch: 395 | Loss: 9.405635523762612e-07\n",
            "Epoch: 396 | Loss: 9.27525661609252e-07\n",
            "Epoch: 397 | Loss: 9.141343184637662e-07\n",
            "Epoch: 398 | Loss: 9.009511927615677e-07\n",
            "Epoch: 399 | Loss: 8.881918347469764e-07\n",
            "Epoch: 400 | Loss: 8.750344022701029e-07\n",
            "Epoch: 401 | Loss: 8.62460694861511e-07\n",
            "Epoch: 402 | Loss: 8.500321655446896e-07\n",
            "Epoch: 403 | Loss: 8.380125109397341e-07\n",
            "Epoch: 404 | Loss: 8.2623728303588e-07\n",
            "Epoch: 405 | Loss: 8.143354648382228e-07\n",
            "Epoch: 406 | Loss: 8.022600468393648e-07\n",
            "Epoch: 407 | Loss: 7.905841812316794e-07\n",
            "Epoch: 408 | Loss: 7.795075021022058e-07\n",
            "Epoch: 409 | Loss: 7.684582783440419e-07\n",
            "Epoch: 410 | Loss: 7.569817626063013e-07\n",
            "Epoch: 411 | Loss: 7.464955160685349e-07\n",
            "Epoch: 412 | Loss: 7.355834554800822e-07\n",
            "Epoch: 413 | Loss: 7.248511906254862e-07\n",
            "Epoch: 414 | Loss: 7.145422387111466e-07\n",
            "Epoch: 415 | Loss: 7.047451617836487e-07\n",
            "Epoch: 416 | Loss: 6.94095660946914e-07\n",
            "Epoch: 417 | Loss: 6.844404651928926e-07\n",
            "Epoch: 418 | Loss: 6.744238021383353e-07\n",
            "Epoch: 419 | Loss: 6.650964223808842e-07\n",
            "Epoch: 420 | Loss: 6.552227773681807e-07\n",
            "Epoch: 421 | Loss: 6.457964900619118e-07\n",
            "Epoch: 422 | Loss: 6.366699949467147e-07\n",
            "Epoch: 423 | Loss: 6.274713655329833e-07\n",
            "Epoch: 424 | Loss: 6.181101070978912e-07\n",
            "Epoch: 425 | Loss: 6.092734565754654e-07\n",
            "Epoch: 426 | Loss: 6.00544865392294e-07\n",
            "Epoch: 427 | Loss: 5.920583703300508e-07\n",
            "Epoch: 428 | Loss: 5.837658250129607e-07\n",
            "Epoch: 429 | Loss: 5.751788307861716e-07\n",
            "Epoch: 430 | Loss: 5.669619440595852e-07\n",
            "Epoch: 431 | Loss: 5.588042881754518e-07\n",
            "Epoch: 432 | Loss: 5.507486662281735e-07\n",
            "Epoch: 433 | Loss: 5.430515557236504e-07\n",
            "Epoch: 434 | Loss: 5.350682954485819e-07\n",
            "Epoch: 435 | Loss: 5.274399086374615e-07\n",
            "Epoch: 436 | Loss: 5.199078714213101e-07\n",
            "Epoch: 437 | Loss: 5.12430062826752e-07\n",
            "Epoch: 438 | Loss: 5.04676108903368e-07\n",
            "Epoch: 439 | Loss: 4.979242476110812e-07\n",
            "Epoch: 440 | Loss: 4.905662081000628e-07\n",
            "Epoch: 441 | Loss: 4.835862910113065e-07\n",
            "Epoch: 442 | Loss: 4.7665639613114763e-07\n",
            "Epoch: 443 | Loss: 4.697765234595863e-07\n",
            "Epoch: 444 | Loss: 4.629466729966225e-07\n",
            "Epoch: 445 | Loss: 4.562458002510539e-07\n",
            "Epoch: 446 | Loss: 4.4970994395043817e-07\n",
            "Epoch: 447 | Loss: 4.434152742760489e-07\n",
            "Epoch: 448 | Loss: 4.3704955032808357e-07\n",
            "Epoch: 449 | Loss: 4.307298695493955e-07\n",
            "Epoch: 450 | Loss: 4.244562319399847e-07\n",
            "Epoch: 451 | Loss: 4.183046371508681e-07\n",
            "Epoch: 452 | Loss: 4.1242060433432925e-07\n",
            "Epoch: 453 | Loss: 4.065785219609097e-07\n",
            "Epoch: 454 | Loss: 4.007416123386065e-07\n",
            "Epoch: 455 | Loss: 3.9476509527958115e-07\n",
            "Epoch: 456 | Loss: 3.893394477927359e-07\n",
            "Epoch: 457 | Loss: 3.8362804843927734e-07\n",
            "Epoch: 458 | Loss: 3.7795894058945123e-07\n",
            "Epoch: 459 | Loss: 3.728996489371639e-07\n",
            "Epoch: 460 | Loss: 3.674163053801749e-07\n",
            "Epoch: 461 | Loss: 3.621135533649067e-07\n",
            "Epoch: 462 | Loss: 3.567107000890246e-07\n",
            "Epoch: 463 | Loss: 3.5176162782590836e-07\n",
            "Epoch: 464 | Loss: 3.466075213509612e-07\n",
            "Epoch: 465 | Loss: 3.415598825995403e-07\n",
            "Epoch: 466 | Loss: 3.367174485902069e-07\n",
            "Epoch: 467 | Loss: 3.317427967886033e-07\n",
            "Epoch: 468 | Loss: 3.27236421071575e-07\n",
            "Epoch: 469 | Loss: 3.2249710102405515e-07\n",
            "Epoch: 470 | Loss: 3.1785822329766233e-07\n",
            "Epoch: 471 | Loss: 3.131874564132886e-07\n",
            "Epoch: 472 | Loss: 3.088746325374814e-07\n",
            "Epoch: 473 | Loss: 3.0427059982685023e-07\n",
            "Epoch: 474 | Loss: 2.999241246470774e-07\n",
            "Epoch: 475 | Loss: 2.957038418571756e-07\n",
            "Epoch: 476 | Loss: 2.914192123171233e-07\n",
            "Epoch: 477 | Loss: 2.872594109248894e-07\n",
            "Epoch: 478 | Loss: 2.830366270245577e-07\n",
            "Epoch: 479 | Loss: 2.7893730702999164e-07\n",
            "Epoch: 480 | Loss: 2.7498981580720283e-07\n",
            "Epoch: 481 | Loss: 2.7119153855892364e-07\n",
            "Epoch: 482 | Loss: 2.6729941282610525e-07\n",
            "Epoch: 483 | Loss: 2.6331605340601527e-07\n",
            "Epoch: 484 | Loss: 2.5957001525966916e-07\n",
            "Epoch: 485 | Loss: 2.5576247253411566e-07\n",
            "Epoch: 486 | Loss: 2.520707766962005e-07\n",
            "Epoch: 487 | Loss: 2.4840602463882533e-07\n",
            "Epoch: 488 | Loss: 2.4491146177751943e-07\n",
            "Epoch: 489 | Loss: 2.4129923303917167e-07\n",
            "Epoch: 490 | Loss: 2.3805392856957042e-07\n",
            "Epoch: 491 | Loss: 2.3449274522135966e-07\n",
            "Epoch: 492 | Loss: 2.3115421754482668e-07\n",
            "Epoch: 493 | Loss: 2.2778368702347507e-07\n",
            "Epoch: 494 | Loss: 2.2471368765764055e-07\n",
            "Epoch: 495 | Loss: 2.211720584455179e-07\n",
            "Epoch: 496 | Loss: 2.1814707906742115e-07\n",
            "Epoch: 497 | Loss: 2.1514301806746516e-07\n",
            "Epoch: 498 | Loss: 2.117315034411149e-07\n",
            "Epoch: 499 | Loss: 2.0895805619147723e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d74Y17BrbeyB",
        "outputId": "bada1ea6-cbb6-4ae0-db9a-1f5e59378c83"
      },
      "source": [
        "# After training\n",
        "hour_var = Variable(torch.Tensor([[4.0]]))\n",
        "print(\"predict (after training)\", 4, model.forward(hour_var).data[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (after training) 4 tensor(7.9995)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xov0gALme2s3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}